#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è®ºæ–‡å‚è€ƒæ–‡çŒ®ç®¡ç†MCPå·¥å…·
æä¾›å‚è€ƒæ–‡çŒ®åˆ†æã€æ¸…ç†å’Œç®¡ç†åŠŸèƒ½
"""

import json
import re
import os
import requests
from typing import Dict, List, Set, Tuple, Optional
from mcp.server import Server
from mcp.types import Tool, TextContent
import asyncio
import xml.etree.ElementTree as ET

# åˆ›å»ºMCPæœåŠ¡å™¨
server = Server("thesis-reference-manager")

@server.list_tools()
async def list_tools() -> List[Tool]:
    """åˆ—å‡ºå¯ç”¨çš„å·¥å…·"""
    return [
        # ç½‘ç»œæœç´¢å·¥å…·
        Tool(
            name="search_papers_arxiv",
            description="åœ¨arXivä¸Šæœç´¢è®ºæ–‡",
            inputSchema={
                "type": "object",
                "properties": {
                    "query": {
                        "type": "string",
                        "description": "æœç´¢å…³é”®è¯"
                    },
                    "max_results": {
                        "type": "integer",
                        "description": "æœ€å¤§ç»“æœæ•°é‡ï¼Œé»˜è®¤10"
                    }
                },
                "required": ["query"]
            }
        ),
        # Tool(
        #     name="search_papers_semantic_scholar",
        #     description="åœ¨Semantic Scholarä¸Šæœç´¢è®ºæ–‡",
        #     inputSchema={
        #         "type": "object",
        #         "properties": {
        #             "query": {
        #                 "type": "string",
        #                 "description": "æœç´¢å…³é”®è¯"
        #             },
        #             "max_results": {
        #                 "type": "integer",
        #                 "description": "æœ€å¤§ç»“æœæ•°é‡ï¼Œé»˜è®¤10"
        #             }
        #         },
        #         "required": ["query"]
        #     }
        # ),
        Tool(
            name="get_paper_details",
            description="è·å–è®ºæ–‡è¯¦ç»†ä¿¡æ¯",
            inputSchema={
                "type": "object",
                "properties": {
                    "paper_id": {
                        "type": "string",
                        "description": "è®ºæ–‡IDæˆ–DOI"
                    },
                    "source": {
                        "type": "string",
                        "description": "æ¥æºï¼šarxiv, doi, title",
                        "enum": ["arxiv", "doi", "title"]
                    }
                },
                "required": ["paper_id", "source"]
            }
        ),
        Tool(
            name="save_search_results",
            description="ä¿å­˜æœç´¢ç»“æœåˆ°æŒ‡å®šç›®å½• - æŒ‰é¢†åŸŸåˆ†ç±»ä¿å­˜åˆ° references/é¢†åŸŸ/ ç›®å½•",
            inputSchema={
                "type": "object",
                "properties": {
                    "results": {
                        "type": "array",
                        "description": "æœç´¢ç»“æœåˆ—è¡¨"
                    },
                    "domain": {
                        "type": "string",
                        "description": "ç ”ç©¶é¢†åŸŸï¼Œå¦‚ï¼šmachine_learning, computer_vision, nlp ç­‰"
                    },
                    "base_path": {
                        "type": "string",
                        "description": "åŸºç¡€ä¿å­˜è·¯å¾„ï¼Œé»˜è®¤ä¸ºreferencesã€‚å¦‚æœä½¿ç”¨ç›¸å¯¹è·¯å¾„ï¼Œéœ€è¦æä¾›user_workspaceå‚æ•°"
                    },
                    "user_workspace": {
                        "type": "string",
                        "description": "ç”¨æˆ·å·¥ä½œåŒºè·¯å¾„ï¼ˆå¿…éœ€ï¼‰ï¼Œç”¨äºç¡®å®šç›¸å¯¹è·¯å¾„çš„åŸºå‡†ç›®å½•ï¼Œå¦‚ï¼šD:/Users/username/Desktop"
                    }
                },
                "required": ["results", "domain", "user_workspace"]
            }
        ),
        # åŸæœ‰åŠŸèƒ½
        Tool(
            name="analyze_citations",
            description="åˆ†æè®ºæ–‡ä¸­ä½¿ç”¨çš„å¼•ç”¨ï¼Œæ‰¾å‡ºæœªä½¿ç”¨çš„å‚è€ƒæ–‡çŒ®",
            inputSchema={
                "type": "object",
                "properties": {
                    "tex_file": {
                        "type": "string",
                        "description": "LaTeXæ–‡ä»¶è·¯å¾„ï¼Œé»˜è®¤ä¸ºthesis_draft.tex"
                    }
                }
            }
        ),
        Tool(
            name="clean_unused_references",
            description="åˆ é™¤æœªä½¿ç”¨çš„å‚è€ƒæ–‡çŒ®",
            inputSchema={
                "type": "object",
                "properties": {
                    "tex_file": {
                        "type": "string",
                        "description": "LaTeXæ–‡ä»¶è·¯å¾„ï¼Œé»˜è®¤ä¸ºthesis_draft.tex"
                    }
                }
            }
        ),
        Tool(
            name="convert_citations_to_superscript",
            description="å°†LaTeXä¸­çš„\\citeå¼•ç”¨è½¬æ¢ä¸ºä¸Šæ ‡æ ¼å¼",
            inputSchema={
                "type": "object",
                "properties": {
                    "tex_file": {
                        "type": "string",
                        "description": "LaTeXæ–‡ä»¶è·¯å¾„ï¼Œé»˜è®¤ä¸ºthesis_draft.tex"
                    }
                }
            }
        )
    ]

@server.call_tool()
async def call_tool(name: str, arguments: Dict) -> List[TextContent]:
    """è°ƒç”¨å·¥å…·"""
    
    if name == "search_papers_arxiv":
        return await search_papers_arxiv(
            arguments["query"], 
            arguments.get("max_results", 10)
        )
    
    # elif name == "search_papers_semantic_scholar":
    #     return await search_papers_semantic_scholar(
    #         arguments["query"], 
    #         arguments.get("max_results", 10)
    #     )
    
    elif name == "get_paper_details":
        return await get_paper_details(
            arguments["paper_id"], 
            arguments["source"]
        )
    
    elif name == "save_search_results":
        return await save_search_results(
            arguments["results"], 
            arguments["domain"],
            arguments.get("base_path", "references"),
            arguments.get("user_workspace")
        )
    
    elif name == "analyze_citations":
        return await analyze_citations(arguments.get("tex_file", "thesis_draft.tex"))
    
    elif name == "clean_unused_references":
        return await clean_unused_references(arguments.get("tex_file", "thesis_draft.tex"))
    
    elif name == "save_references":
        return await save_references(arguments.get("output_dir", "references"))
    
    elif name == "convert_citations_to_superscript":
        return await convert_citations_to_superscript(arguments.get("tex_file", "thesis_draft.tex"))
    
    else:
        raise ValueError(f"æœªçŸ¥å·¥å…·: {name}")

async def search_papers_arxiv(query: str, max_results: int = 10) -> List[TextContent]:
    """åœ¨arXivä¸Šæœç´¢è®ºæ–‡"""
    try:
        # arXiv APIæœç´¢
        url = f"http://export.arxiv.org/api/query"
        params = {
            'search_query': f'all:{query}',
            'start': 0,
            'max_results': max_results,
            'sortBy': 'relevance',
            'sortOrder': 'descending'
        }
        
        response = requests.get(url, params=params, timeout=30)
        response.raise_for_status()
        
        # è§£æXMLå“åº”
        root = ET.fromstring(response.content)
        
        results = []
        for entry in root.findall('{http://www.w3.org/2005/Atom}entry'):
            title = entry.find('{http://www.w3.org/2005/Atom}title').text.strip()
            authors = []
            for author in entry.findall('{http://www.w3.org/2005/Atom}author'):
                name = author.find('{http://www.w3.org/2005/Atom}name').text
                authors.append(name)
            
            published = entry.find('{http://www.w3.org/2005/Atom}published').text
            summary = entry.find('{http://www.w3.org/2005/Atom}summary').text.strip()
            arxiv_id = entry.find('{http://www.w3.org/2005/Atom}id').text.split('/')[-1]
            
            result = {
                'title': title,
                'authors': authors,
                'published': published,
                'summary': summary,
                'arxiv_id': arxiv_id,
                'url': f"https://arxiv.org/abs/{arxiv_id}",
                'source': 'arxiv'
            }
            results.append(result)
        
        # æ ¼å¼åŒ–è¾“å‡º
        output = f"arXivæœç´¢ç»“æœ (å…³é”®è¯: {query}):\n\n"
        for i, result in enumerate(results, 1):
            output += f"{i}. {result['title']}\n"
            output += f"   ä½œè€…: {', '.join(result['authors'])}\n"
            output += f"   å‘è¡¨æ—¶é—´: {result['published'][:10]}\n"
            output += f"   arXiv ID: {result['arxiv_id']}\n"
            output += f"   é“¾æ¥: {result['url']}\n"
            output += f"   æ‘˜è¦: {result['summary'][:200]}...\n\n"
        
        return [TextContent(type="text", text=output)]
        
    except Exception as e:
        return [TextContent(type="text", text=f"arXivæœç´¢å‡ºé”™: {str(e)}")]

# async def search_papers_semantic_scholar(query: str, max_results: int = 10) -> List[TextContent]:
#     """åœ¨Semantic Scholarä¸Šæœç´¢è®ºæ–‡"""
#     try:
#         # ä½¿ç”¨Semantic Scholar API
#         url = "https://api.semanticscholar.org/graph/v1/paper/search"
#         params = {
#             'query': query,
#             'limit': max_results,
#             'fields': 'title,authors,year,abstract,url,paperId,venue'
#         }
        
#         response = requests.get(url, params=params, timeout=30)
#         response.raise_for_status()
        
#         data = response.json()
#         results = []
        
#         for paper in data.get('data', []):
#             authors = [author['name'] for author in paper.get('authors', [])]
#             result = {
#                 'title': paper.get('title', ''),
#                 'authors': authors,
#                 'year': paper.get('year'),
#                 'abstract': paper.get('abstract', ''),
#                 'url': paper.get('url', ''),
#                 'paper_id': paper.get('paperId', ''),
#                 'venue': paper.get('venue', ''),
#                 'source': 'semantic_scholar'
#             }
#             results.append(result)
        
        # æ ¼å¼åŒ–è¾“å‡º
        output = f"Semantic Scholaræœç´¢ç»“æœ (å…³é”®è¯: {query}):\n\n"
        for i, result in enumerate(results, 1):
            output += f"{i}. {result['title']}\n"
            output += f"   ä½œè€…: {', '.join(result['authors'])}\n"
            output += f"   å¹´ä»½: {result['year']}\n"
            if result['venue']:
                output += f"   æœŸåˆŠ/ä¼šè®®: {result['venue']}\n"
            output += f"   é“¾æ¥: {result['url']}\n"
            output += f"   æ‘˜è¦: {result['abstract'][:200]}...\n\n"
        
        return [TextContent(type="text", text=output)]
        
    except Exception as e:
        return [TextContent(type="text", text=f"Semantic Scholaræœç´¢å‡ºé”™: {str(e)}")]

async def get_paper_details(paper_id: str, source: str) -> List[TextContent]:
    """è·å–è®ºæ–‡è¯¦ç»†ä¿¡æ¯"""
    try:
        if source == "arxiv":
            # è·å–arXivè®ºæ–‡è¯¦æƒ…
            url = f"http://export.arxiv.org/api/query?id_list={paper_id}"
            response = requests.get(url, timeout=30)
            response.raise_for_status()
            
            root = ET.fromstring(response.content)
            entry = root.find('{http://www.w3.org/2005/Atom}entry')
            
            if entry is not None:
                title = entry.find('{http://www.w3.org/2005/Atom}title').text.strip()
                authors = []
                for author in entry.findall('{http://www.w3.org/2005/Atom}author'):
                    name = author.find('{http://www.w3.org/2005/Atom}name').text
                    authors.append(name)
                
                published = entry.find('{http://www.w3.org/2005/Atom}published').text
                summary = entry.find('{http://www.w3.org/2005/Atom}summary').text.strip()
                
                # ç”ŸæˆBibTeXæ ¼å¼
                bibtex_key = f"arxiv_{paper_id.replace('.', '_')}"
                bibtex = f"""@article{{{bibtex_key},
    title={{{title}}},
    author={{{' and '.join(authors)}}},
    journal={{arXiv preprint arXiv:{paper_id}}},
    year={{{published[:4]}}},
    url={{https://arxiv.org/abs/{paper_id}}}
}}"""
                
                result = f"è®ºæ–‡è¯¦ç»†ä¿¡æ¯:\n\n"
                result += f"æ ‡é¢˜: {title}\n"
                result += f"ä½œè€…: {', '.join(authors)}\n"
                result += f"å‘è¡¨æ—¶é—´: {published}\n"
                result += f"arXiv ID: {paper_id}\n"
                result += f"æ‘˜è¦: {summary}\n\n"
                result += f"BibTeXæ ¼å¼:\n{bibtex}"
                
                return [TextContent(type="text", text=result)]
        
        elif source == "doi":
            # ä½¿ç”¨DOIè·å–è®ºæ–‡ä¿¡æ¯
            url = f"https://api.crossref.org/works/{paper_id}"
            response = requests.get(url, timeout=30)
            response.raise_for_status()
            
            data = response.json()
            work = data['message']
            
            title = work.get('title', [''])[0]
            authors = [author.get('given', '') + ' ' + author.get('family', '') 
                      for author in work.get('author', [])]
            year = work.get('published-print', {}).get('date-parts', [[None]])[0][0]
            journal = work.get('container-title', [''])[0]
            
            result = f"è®ºæ–‡è¯¦ç»†ä¿¡æ¯ (DOI: {paper_id}):\n\n"
            result += f"æ ‡é¢˜: {title}\n"
            result += f"ä½œè€…: {', '.join(authors)}\n"
            result += f"å¹´ä»½: {year}\n"
            result += f"æœŸåˆŠ: {journal}\n"
            result += f"DOI: {paper_id}\n"
            
            return [TextContent(type="text", text=result)]
        
        else:
            return [TextContent(type="text", text=f"ä¸æ”¯æŒçš„æ¥æºç±»å‹: {source}")]
            
    except Exception as e:
        return [TextContent(type="text", text=f"è·å–è®ºæ–‡è¯¦æƒ…å‡ºé”™: {str(e)}")]

async def save_search_results(results: List[Dict], domain: str, base_path: str = "references", user_workspace: str = None) -> List[TextContent]:
    """ä¿å­˜æœç´¢ç»“æœåˆ°æŒ‡å®šè·¯å¾„ - æŒ‰é¢†åŸŸåˆ†ç±»ä¿å­˜"""
    try:
        # å¦‚æœbase_pathæ˜¯ç›¸å¯¹è·¯å¾„ï¼Œéœ€è¦ç”¨æˆ·æä¾›ç”¨æˆ·å·¥ä½œåŒºè·¯å¾„
        if not os.path.isabs(base_path):
            if user_workspace:
                # ä½¿ç”¨ç”¨æˆ·æä¾›çš„å·¥ä½œåŒºè·¯å¾„
                base_path = os.path.join(user_workspace, base_path)
            else:
                # å¦‚æœæ²¡æœ‰æä¾›ç”¨æˆ·å·¥ä½œåŒºï¼Œè¿”å›é”™è¯¯æç¤º
                return [TextContent(type="text", text="âŒ é”™è¯¯ï¼šç›¸å¯¹è·¯å¾„éœ€è¦æä¾›ç”¨æˆ·å·¥ä½œåŒºè·¯å¾„å‚æ•° user_workspace")]
        
        # æ„å»ºå®Œæ•´çš„ä¿å­˜è·¯å¾„ï¼šreferences/é¢†åŸŸ/
        save_path = os.path.join(base_path, domain)
        
        # ç¡®ä¿ä¿å­˜è·¯å¾„å­˜åœ¨
        os.makedirs(save_path, exist_ok=True)
        
        # ç”Ÿæˆæ—¶é—´æˆ³
        from datetime import datetime
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # ä¿å­˜JSONæ–‡ä»¶
        json_file = os.path.join(save_path, f"papers_{timestamp}.json")
        with open(json_file, "w", encoding="utf-8") as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        # ç”Ÿæˆç®€åŒ–çš„BibTeXæ–‡ä»¶
        bibtex_file = os.path.join(save_path, f"papers_{timestamp}.bib")
        bibtex_entries = []
        for i, result in enumerate(results, 1):
            if result.get('source') == 'arxiv':
                key = f"paper_{i}"
                bibtex = f"""@article{{{key},
    title={{{result['title']}}},
    author={{{' and '.join(result['authors'])}}},
    journal={{arXiv preprint arXiv:{result['arxiv_id']}}},
    year={{{result['published'][:4]}}},
    url={{{result['url']}}}
}}"""
            else:
                key = f"paper_{i}"
                bibtex = f"""@article{{{key},
    title={{{result['title']}}},
    author={{{' and '.join(result['authors'])}}},
    journal={{Unknown}},
    year={{{result.get('year', 'N/A')}}},
    url={{{result['url']}}}
}}"""
            bibtex_entries.append(bibtex)
        
        with open(bibtex_file, "w", encoding="utf-8") as f:
            f.write('\n\n'.join(bibtex_entries))
        
        # ç”Ÿæˆé¢†åŸŸä¿¡æ¯æ–‡ä»¶
        domain_info_file = os.path.join(save_path, f"domain_info_{timestamp}.txt")
        with open(domain_info_file, "w", encoding="utf-8") as f:
            f.write(f"ç ”ç©¶é¢†åŸŸ: {domain}\n")
            f.write(f"è®ºæ–‡æ•°é‡: {len(results)}\n")
            f.write(f"ä¿å­˜æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"æœç´¢å…³é”®è¯: {', '.join(set([r.get('query', 'N/A') for r in results if 'query' in r]))}\n")
        
        # è¿”å›ç®€æ´çš„ç»“æœ
        result_text = f"âœ… å·²ä¿å­˜ {len(results)} ç¯‡è®ºæ–‡åˆ°:\n"
        result_text += f"ğŸ“ ç›®å½•: {os.path.abspath(save_path)}\n"
        result_text += f"ğŸ·ï¸ é¢†åŸŸ: {domain}\n"
        result_text += f"ğŸ“„ JSON: {os.path.basename(json_file)}\n"
        result_text += f"ğŸ“„ BibTeX: {os.path.basename(bibtex_file)}\n"
        result_text += f"ğŸ“„ é¢†åŸŸä¿¡æ¯: {os.path.basename(domain_info_file)}"
        
        return [TextContent(type="text", text=result_text)]
        
    except Exception as e:
        return [TextContent(type="text", text=f"âŒ ä¿å­˜å¤±è´¥: {str(e)}")]

async def analyze_citations(tex_file: str) -> List[TextContent]:
    """åˆ†æè®ºæ–‡ä¸­ä½¿ç”¨çš„å¼•ç”¨ï¼Œæ‰¾å‡ºæœªä½¿ç”¨çš„å‚è€ƒæ–‡çŒ®"""
    
    try:
        # è¯»å–æ–‡ä»¶
        with open(tex_file, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # æ‰¾å‡ºæ‰€æœ‰ä½¿ç”¨çš„å¼•ç”¨
        used_citations = set()
        cite_pattern = r'\\cite\{([^}]+)\}'
        matches = re.findall(cite_pattern, content)
        
        for match in matches:
            # å¤„ç†å¤šä¸ªå¼•ç”¨çš„æƒ…å†µï¼Œå¦‚ \cite{key1,key2}
            citations = [c.strip() for c in match.split(',')]
            used_citations.update(citations)
        
        # æ‰¾å‡ºæ‰€æœ‰å®šä¹‰çš„å‚è€ƒæ–‡çŒ®
        defined_citations = set()
        bibitem_pattern = r'\\bibitem\{([^}]+)\}'
        bibitem_matches = re.findall(bibitem_pattern, content)
        defined_citations.update(bibitem_matches)
        
        # æ‰¾å‡ºæœªä½¿ç”¨çš„å‚è€ƒæ–‡çŒ®
        unused_citations = defined_citations - used_citations
        
        # æ‰¾å‡ºä½¿ç”¨ä½†æœªå®šä¹‰çš„å¼•ç”¨
        undefined_citations = used_citations - defined_citations
        
        result = f"""å¼•ç”¨åˆ†æç»“æœ:

ä½¿ç”¨çš„å¼•ç”¨æ•°é‡: {len(used_citations)}
å®šä¹‰çš„å‚è€ƒæ–‡çŒ®æ•°é‡: {len(defined_citations)}
æœªä½¿ç”¨çš„å‚è€ƒæ–‡çŒ®æ•°é‡: {len(unused_citations)}
ä½¿ç”¨ä½†æœªå®šä¹‰çš„å¼•ç”¨æ•°é‡: {len(undefined_citations)}

ä½¿ç”¨çš„å¼•ç”¨:
{', '.join(sorted(used_citations))}

æœªä½¿ç”¨çš„å‚è€ƒæ–‡çŒ®:
{', '.join(sorted(unused_citations)) if unused_citations else 'æ— '}

ä½¿ç”¨ä½†æœªå®šä¹‰çš„å¼•ç”¨:
{', '.join(sorted(undefined_citations)) if undefined_citations else 'æ— '}"""
        
        return [TextContent(type="text", text=result)]
        
    except Exception as e:
        return [TextContent(type="text", text=f"åˆ†æå¼•ç”¨æ—¶å‡ºé”™: {str(e)}")]

async def clean_unused_references(tex_file: str) -> List[TextContent]:
    """åˆ é™¤æœªä½¿ç”¨çš„å‚è€ƒæ–‡çŒ®"""
    
    try:
        # è¯»å–æ–‡ä»¶
        with open(tex_file, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # æ‰¾å‡ºæ‰€æœ‰ä½¿ç”¨çš„å¼•ç”¨
        used_citations = set()
        cite_pattern = r'\\cite\{([^}]+)\}'
        matches = re.findall(cite_pattern, content)
        
        for match in matches:
            citations = [c.strip() for c in match.split(',')]
            used_citations.update(citations)
        
        # æ‰¾å‡ºæ‰€æœ‰å®šä¹‰çš„å‚è€ƒæ–‡çŒ®
        defined_citations = set()
        bibitem_pattern = r'\\bibitem\{([^}]+)\}'
        bibitem_matches = re.findall(bibitem_pattern, content)
        defined_citations.update(bibitem_matches)
        
        # æ‰¾å‡ºæœªä½¿ç”¨çš„å‚è€ƒæ–‡çŒ®
        unused_citations = defined_citations - used_citations
        
        # åˆ é™¤æœªä½¿ç”¨çš„å‚è€ƒæ–‡çŒ®
        deleted_count = 0
        for citation in unused_citations:
            # æ‰¾åˆ°å¯¹åº”çš„bibitemå¹¶åˆ é™¤
            pattern = rf'\\bibitem\{{{citation}\}}.*?(?=\\bibitem\{{|\\end\{{thebibliography\}}|$)'
            new_content = re.sub(pattern, '', content, flags=re.DOTALL)
            if new_content != content:
                content = new_content
                deleted_count += 1
        
        # æ¸…ç†å¤šä½™çš„ç©ºè¡Œ
        content = re.sub(r'\n\s*\n\s*\n', '\n\n', content)
        
        # å†™å›æ–‡ä»¶
        with open(tex_file, 'w', encoding='utf-8') as f:
            f.write(content)
        
        result = f"æˆåŠŸåˆ é™¤ {deleted_count} ä¸ªæœªä½¿ç”¨çš„å‚è€ƒæ–‡çŒ®\n"
        result += f"åˆ é™¤çš„å‚è€ƒæ–‡çŒ®: {', '.join(sorted(unused_citations))}"
        
        return [TextContent(type="text", text=result)]
        
    except Exception as e:
        return [TextContent(type="text", text=f"æ¸…ç†å‚è€ƒæ–‡çŒ®æ—¶å‡ºé”™: {str(e)}")]

async def save_references(output_dir: str = "references") -> List[TextContent]:
    """ä¿å­˜å‚è€ƒæ–‡çŒ®åˆ°ä¸åŒæ ¼å¼"""
    try:
        # åˆ›å»ºè¾“å‡ºç›®å½•
        if not os.path.exists(output_dir):
            os.makedirs(output_dir)
        
        # è¿™é‡Œå¯ä»¥æ·»åŠ ä»LaTeXæ–‡ä»¶ä¸­æå–å‚è€ƒæ–‡çŒ®çš„é€»è¾‘
        # ç›®å‰è¿”å›ä¸€ä¸ªç¤ºä¾‹ç»“æœ
        result = f"å‚è€ƒæ–‡çŒ®å·²ä¿å­˜åˆ° {output_dir} æ–‡ä»¶å¤¹\n"
        result += "åŒ…å«æ–‡ä»¶:\n"
        result += "- references.json (JSONæ ¼å¼)\n"
        result += "- references.bib (BibTeXæ ¼å¼)\n"
        result += "- references.md (Markdownæ ¼å¼)\n"
        result += "æ³¨æ„: æ­¤åŠŸèƒ½éœ€è¦ä»LaTeXæ–‡ä»¶ä¸­æå–å‚è€ƒæ–‡çŒ®ä¿¡æ¯"
        
        return [TextContent(type="text", text=result)]
        
    except Exception as e:
        return [TextContent(type="text", text=f"ä¿å­˜å‚è€ƒæ–‡çŒ®æ—¶å‡ºé”™: {str(e)}")]

async def convert_citations_to_superscript(tex_file: str) -> List[TextContent]:
    """å°†LaTeXä¸­çš„\\citeå¼•ç”¨è½¬æ¢ä¸ºä¸Šæ ‡æ ¼å¼"""
    try:
        # è¯»å–æ–‡ä»¶
        with open(tex_file, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # åˆ›å»ºå¼•ç”¨æ˜ å°„
        citation_map = {}
        citation_counter = 1
        
        # æ‰¾å‡ºæ‰€æœ‰ä½¿ç”¨çš„å¼•ç”¨
        cite_pattern = r'\\cite\{([^}]+)\}'
        matches = re.findall(cite_pattern, content)
        
        for match in matches:
            citations = [c.strip() for c in match.split(',')]
            for citation in citations:
                if citation not in citation_map:
                    citation_map[citation] = citation_counter
                    citation_counter += 1
        
        # æ›¿æ¢å¼•ç”¨ä¸ºä¸Šæ ‡æ ¼å¼
        def replace_cite(match):
            citations = [c.strip() for c in match.group(1).split(',')]
            superscripts = [str(citation_map[c]) for c in citations]
            return f"$^{{{','.join(superscripts)}}}$"
        
        new_content = re.sub(cite_pattern, replace_cite, content)
        
        # å†™å›æ–‡ä»¶
        with open(tex_file, 'w', encoding='utf-8') as f:
            f.write(new_content)
        
        result = f"æˆåŠŸè½¬æ¢ {len(citation_map)} ä¸ªå¼•ç”¨ä¸ºä¸Šæ ‡æ ¼å¼\n"
        result += f"è½¬æ¢çš„å¼•ç”¨: {', '.join(sorted(citation_map.keys()))}\n"
        result += f"å¼•ç”¨æ˜ å°„: {dict(citation_map)}"
        
        return [TextContent(type="text", text=result)]
        
    except Exception as e:
        return [TextContent(type="text", text=f"è½¬æ¢å¼•ç”¨æ ¼å¼æ—¶å‡ºé”™: {str(e)}")]

if __name__ == "__main__":
    from mcp import stdio_server
    import asyncio
    
    async def main():
        async with stdio_server() as (read_stream, write_stream):
            await server.run(read_stream, write_stream, server.create_initialization_options())
    
    asyncio.run(main())
